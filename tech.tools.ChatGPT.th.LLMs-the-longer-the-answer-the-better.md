---
id: gh84n8029ypmdbwixhunwkq
title: LLMs-the-longer-the-answer-the-better
desc: ''
updated: 1687653657079
created: 1687653640434
---

## Although this isn’t fully proven, the longer the answer, the better.

> Although we aren’t fully prepared to explain why CoT works, researchers are considering two possible options:
> 
> - “Models need tokens to think”: As Andrej Karpathy described in the last Microsoft conference, the more tokens the answer has, the stronger the answer is, as the next word prediction is grounded on more data.
> - More computational power: Alternatively, the longer the answer, the more computational effort the model is putting into answering your question. More computational power means better answers.
> 
> Bottom line, although this isn’t fully proven, the longer the answer, the better. - [ref](https://medium.com/@ignacio.de.gregorio.noblejas/tree-of-thoughts-f07aa4093870)

